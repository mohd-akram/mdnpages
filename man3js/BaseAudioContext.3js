.\" Automatically generated by Pandoc 3.1.11
.\"
.TH "BaseAudioContext" "JS" "February 19, 2023" "JavaScript" "JavaScript Reference Manual"
.SH NAME
BaseAudioContext \- BaseAudioContext
.SH SYNOPSIS
The \f[CR]BaseAudioContext\f[R] interface of the Web Audio API acts as a
base definition for online and offline audio\-processing graphs, as
represented by \f[CR]AudioContext\f[R] and
\f[CR]OfflineAudioContext\f[R] respectively.
You wouldn\[cq]t use \f[CR]BaseAudioContext\f[R] directly \[em]
you\[cq]d use its features via one of these two inheriting interfaces.
.PP
A \f[CR]BaseAudioContext\f[R] can be a target of events, therefore it
implements the \f[CR]EventTarget\f[R] interface.
.SH INSTANCE PROPERTIES
.TP
\f[B]BaseAudioContext.audioWorklet\f[R] \f[I](read\-only)\f[R] \f[I](secure context)\f[R]
Returns the \f[CR]AudioWorklet\f[R] object, which can be used to create
and manage \f[CR]AudioNode\f[R]s in which JavaScript code implementing
the \f[CR]AudioWorkletProcessor\f[R] interface are run in the background
to process audio data.
.TP
\f[B]BaseAudioContext.currentTime\f[R] \f[I](read\-only)\f[R]
Returns a double representing an ever\-increasing hardware time in
seconds used for scheduling.
It starts at \f[CR]0\f[R].
.TP
\f[B]BaseAudioContext.destination\f[R] \f[I](read\-only)\f[R]
Returns an \f[CR]AudioDestinationNode\f[R] representing the final
destination of all audio in the context.
It can be thought of as the audio\-rendering device.
.TP
\f[B]BaseAudioContext.listener\f[R] \f[I](read\-only)\f[R]
Returns the \f[CR]AudioListener\f[R] object, used for 3D spatialization.
.TP
\f[B]BaseAudioContext.sampleRate\f[R] \f[I](read\-only)\f[R]
Returns a float representing the sample rate (in samples per second)
used by all nodes in this context.
The sample\-rate of an \f[CR]AudioContext\f[R] cannot be changed.
.TP
\f[B]BaseAudioContext.state\f[R] \f[I](read\-only)\f[R]
Returns the current state of the \f[CR]AudioContext\f[R].
.SH INSTANCE METHODS
\f[I]Also implements methods from the interface\f[R]
\f[CR]EventTarget\f[R].
.TP
\f[B]BaseAudioContext.createAnalyser()\f[R]
Creates an \f[CR]AnalyserNode\f[R], which can be used to expose audio
time and frequency data and for example to create data visualizations.
.TP
\f[B]BaseAudioContext.createBiquadFilter()\f[R]
Creates a \f[CR]BiquadFilterNode\f[R], which represents a second order
filter configurable as several different common filter types:
high\-pass, low\-pass, band\-pass, etc
.TP
\f[B]BaseAudioContext.createBuffer()\f[R]
Creates a new, empty \f[CR]AudioBuffer\f[R] object, which can then be
populated by data and played via an \f[CR]AudioBufferSourceNode\f[R].
.TP
\f[B]BaseAudioContext.createBufferSource()\f[R]
Creates an \f[CR]AudioBufferSourceNode\f[R], which can be used to play
and manipulate audio data contained within an \f[CR]AudioBuffer\f[R]
object.
\f[CR]AudioBuffer\f[R]s are created using
\f[CR]AudioContext.createBuffer()\f[R] or returned by
\f[CR]AudioContext.decodeAudioData()\f[R] when it successfully decodes
an audio track.
.TP
\f[B]BaseAudioContext.createConstantSource()\f[R]
Creates a \f[CR]ConstantSourceNode\f[R] object, which is an audio source
that continuously outputs a monaural (one\-channel) sound signal whose
samples all have the same value.
.TP
\f[B]BaseAudioContext.createChannelMerger()\f[R]
Creates a \f[CR]ChannelMergerNode\f[R], which is used to combine
channels from multiple audio streams into a single audio stream.
.TP
\f[B]BaseAudioContext.createChannelSplitter()\f[R]
Creates a \f[CR]ChannelSplitterNode\f[R], which is used to access the
individual channels of an audio stream and process them separately.
.TP
\f[B]BaseAudioContext.createConvolver()\f[R]
Creates a \f[CR]ConvolverNode\f[R], which can be used to apply
convolution effects to your audio graph, for example a reverberation
effect.
.TP
\f[B]BaseAudioContext.createDelay()\f[R]
Creates a \f[CR]DelayNode\f[R], which is used to delay the incoming
audio signal by a certain amount.
This node is also useful to create feedback loops in a Web Audio API
graph.
.TP
\f[B]BaseAudioContext.createDynamicsCompressor()\f[R]
Creates a \f[CR]DynamicsCompressorNode\f[R], which can be used to apply
acoustic compression to an audio signal.
.TP
\f[B]BaseAudioContext.createGain()\f[R]
Creates a \f[CR]GainNode\f[R], which can be used to control the overall
volume of the audio graph.
.TP
\f[B]BaseAudioContext.createIIRFilter()\f[R]
Creates an \f[CR]IIRFilterNode\f[R], which represents a second order
filter configurable as several different common filter types.
.TP
\f[B]BaseAudioContext.createOscillator()\f[R]
Creates an \f[CR]OscillatorNode\f[R], a source representing a periodic
waveform.
It basically generates a tone.
.TP
\f[B]BaseAudioContext.createPanner()\f[R]
Creates a \f[CR]PannerNode\f[R], which is used to spatialize an incoming
audio stream in 3D space.
.TP
\f[B]BaseAudioContext.createPeriodicWave()\f[R]
Creates a \f[CR]PeriodicWave\f[R], used to define a periodic waveform
that can be used to determine the output of an
\f[CR]OscillatorNode\f[R].
.TP
\f[B]BaseAudioContext.createScriptProcessor()\f[R] \f[I](deprecated)\f[R]
Creates a \f[CR]ScriptProcessorNode\f[R], which can be used for direct
audio processing via JavaScript.
.TP
\f[B]BaseAudioContext.createStereoPanner()\f[R]
Creates a \f[CR]StereoPannerNode\f[R], which can be used to apply stereo
panning to an audio source.
.TP
\f[B]BaseAudioContext.createWaveShaper()\f[R]
Creates a \f[CR]WaveShaperNode\f[R], which is used to implement
non\-linear distortion effects.
.TP
\f[B]BaseAudioContext.decodeAudioData()\f[R]
Asynchronously decodes audio file data contained in an
\f[CR]ArrayBuffer\f[R].
In this case, the \f[CR]ArrayBuffer\f[R] is usually loaded from an
\f[CR]XMLHttpRequest\f[R]\[cq]s \f[CR]response\f[R] attribute after
setting the \f[CR]responseType\f[R] to \f[CR]arraybuffer\f[R].
This method only works on complete files, not fragments of audio files.
.SH EVENTS
.TP
\f[B]statechange\f[R]
Fired when the \f[CR]AudioContext\f[R]\[cq]s state changes due to the
calling of one of the state change methods
(\f[CR]AudioContext.suspend\f[R], \f[CR]AudioContext.resume\f[R], or
\f[CR]AudioContext.close\f[R]).
.SH EXAMPLES
Basic audio context declaration:
.IP
.EX
const audioContext = new AudioContext();
.EE
.PP
Cross browser variant:
.IP
.EX
const AudioContext = window.AudioContext || window.webkitAudioContext;
const audioContext = new AudioContext();

const oscillatorNode = audioContext.createOscillator();
const gainNode = audioContext.createGain();
const finish = audioContext.destination;
.EE
.SH SEE ALSO
.IP \[bu] 2
Using the Web Audio API
.IP \[bu] 2
\f[CR]AudioContext\f[R]
.IP \[bu] 2
\f[CR]OfflineAudioContext\f[R]
