.\" Automatically generated by Pandoc 3.1.11
.\"
.TH "AnalyserNode" "JS" "March 2, 2023" "JavaScript" "JavaScript Reference Manual"
.SH NAME
AnalyserNode \- AnalyserNode
.SH SYNOPSIS
The \f[B]\f[CB]AnalyserNode\f[B]\f[R] interface represents a node able
to provide real\-time frequency and time\-domain analysis information.
It is an \f[CR]AudioNode\f[R] that passes the audio stream unchanged
from the input to the output, but allows you to take the generated data,
process it, and create audio visualizations.
.PP
An \f[CR]AnalyserNode\f[R] has exactly one input and one output.
The node works even if the output is not connected.
[IMAGE: Without modifying the audio stream, the node allows to get the
frequency and time\-domain data associated to it, using a FFT.]
Without modifying the audio stream, the node allows to get the frequency
and time\-domain data associated to it, using a FFT.
Number of inputs
1
Number of outputs
1 (but may be left unconnected)
Channel count mode
\[lq]max\[rq]
Channel count
2
Channel interpretation
\[lq]speakers\[rq]
.SH CONSTRUCTOR
.TP
\f[B]AnalyserNode()\f[R]
Creates a new instance of an \f[CR]AnalyserNode\f[R] object.
.SH INSTANCE PROPERTIES
\f[I]Inherits properties from its parent, \f[CI]AudioNode\f[I]\f[R].
.TP
\f[B]AnalyserNode.fftSize\f[R]
An unsigned long value representing the size of the FFT (\c
.UR https://en.wikipedia.org/wiki/Fast_Fourier_transform
Fast Fourier Transform
.UE \c
) to be used to determine the frequency domain.
.TP
\f[B]AnalyserNode.frequencyBinCount\f[R] \f[I](read\-only)\f[R]
An unsigned long value half that of the FFT size.
This generally equates to the number of data values you will have to
play with for the visualization.
.TP
\f[B]AnalyserNode.minDecibels\f[R]
A double value representing the minimum power value in the scaling range
for the FFT analysis data, for conversion to unsigned byte values \[em]
basically, this specifies the minimum value for the range of results
when using \f[CR]getByteFrequencyData()\f[R].
.TP
\f[B]AnalyserNode.maxDecibels\f[R]
A double value representing the maximum power value in the scaling range
for the FFT analysis data, for conversion to unsigned byte values \[em]
basically, this specifies the maximum value for the range of results
when using \f[CR]getByteFrequencyData()\f[R].
.TP
\f[B]AnalyserNode.smoothingTimeConstant\f[R]
A double value representing the averaging constant with the last
analysis frame \[em] basically, it makes the transition between values
over time smoother.
.SH INSTANCE METHODS
\f[I]Inherits methods from its parent, \f[CI]AudioNode\f[I]\f[R].
.TP
\f[B]AnalyserNode.getFloatFrequencyData()\f[R]
Copies the current frequency data into a \f[CR]Float32Array\f[R] array
passed into it.
.TP
\f[B]AnalyserNode.getByteFrequencyData()\f[R]
Copies the current frequency data into a \f[CR]Uint8Array\f[R] (unsigned
byte array) passed into it.
.TP
\f[B]AnalyserNode.getFloatTimeDomainData()\f[R]
Copies the current waveform, or time\-domain, data into a
\f[CR]Float32Array\f[R] array passed into it.
.TP
\f[B]AnalyserNode.getByteTimeDomainData()\f[R]
Copies the current waveform, or time\-domain, data into a
\f[CR]Uint8Array\f[R] (unsigned byte array) passed into it.
.SH EXAMPLES
.RS
.PP
\f[B]Note:\f[R] See the guide Visualizations with Web Audio API for more
information on creating audio visualizations.
.RE
.SS Basic usage
The following example shows basic usage of an \f[CR]AudioContext\f[R] to
create an \f[CR]AnalyserNode\f[R], then \f[CR]requestAnimationFrame\f[R]
and \f[CR]<canvas>\f[R] to collect time domain data repeatedly and draw
an \[lq]oscilloscope style\[rq] output of the current audio input.
For more complete applied examples/information, check out our \c
.UR https://mdn.github.io/webaudio-examples/voice-change-o-matic/
Voice\-change\-O\-matic
.UE \c
\ demo (see \c
.UR
https://github.com/mdn/webaudio-examples/tree/main/voice-change-o-matic/scripts/app.js#L108-L193
app.js lines 108\-193
.UE \c
\ for relevant code).
.IP
.EX
const audioCtx = new (window.AudioContext || window.webkitAudioContext)();

// \&...

const analyser = audioCtx.createAnalyser();
analyser.fftSize = 2048;

const bufferLength = analyser.frequencyBinCount;
const dataArray = new Uint8Array(bufferLength);
analyser.getByteTimeDomainData(dataArray);

// Connect the source to be analysed
source.connect(analyser);

// Get a canvas defined with ID \[dq]oscilloscope\[dq]
const canvas = document.getElementById(\[dq]oscilloscope\[dq]);
const canvasCtx = canvas.getContext(\[dq]2d\[dq]);

// draw an oscilloscope of the current audio source

function draw() {
  requestAnimationFrame(draw);

  analyser.getByteTimeDomainData(dataArray);

  canvasCtx.fillStyle = \[dq]rgb(200, 200, 200)\[dq];
  canvasCtx.fillRect(0, 0, canvas.width, canvas.height);

  canvasCtx.lineWidth = 2;
  canvasCtx.strokeStyle = \[dq]rgb(0, 0, 0)\[dq];

  canvasCtx.beginPath();

  const sliceWidth = (canvas.width * 1.0) / bufferLength;
  let x = 0;

  for (let i = 0; i < bufferLength; i++) {
    const v = dataArray[i] / 128.0;
    const y = (v * canvas.height) / 2;

    if (i === 0) {
      canvasCtx.moveTo(x, y);
    } else {
      canvasCtx.lineTo(x, y);
    }

    x += sliceWidth;
  }

  canvasCtx.lineTo(canvas.width, canvas.height / 2);
  canvasCtx.stroke();
}

draw();
.EE
.SH SEE ALSO
.IP \[bu] 2
Using the Web Audio API
