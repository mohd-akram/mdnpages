.\" Automatically generated by Pandoc 3.1.12.3
.\"
.TH "OfflineAudioContext" "JS" "November 21, 2023" "JavaScript" "JavaScript Reference Manual"
.SH NAME
OfflineAudioContext \- OfflineAudioContext
.SH SYNOPSIS
The \f[CR]OfflineAudioContext\f[R] interface is an
\f[CR]AudioContext\f[R] interface representing an audio\-processing
graph built from linked together \f[CR]AudioNode\f[R]s.
In contrast with a standard \f[CR]AudioContext\f[R], an
\f[CR]OfflineAudioContext\f[R] doesn\[cq]t render the audio to the
device hardware; instead, it generates it, as fast as it can, and
outputs the result to an \f[CR]AudioBuffer\f[R].
.SH CONSTRUCTOR
.TP
\f[B]OfflineAudioContext()\f[R]
Creates a new \f[CR]OfflineAudioContext\f[R] instance.
.SH INSTANCE PROPERTIES
\f[I]Also inherits properties from its parent interface,
\f[CI]BaseAudioContext\f[I].\f[R]
.TP
\f[B]OfflineAudioContext.length\f[R] \f[I](read\-only)\f[R]
An integer representing the size of the buffer in sample\-frames.
.SH INSTANCE METHODS
\f[I]Also inherits methods from its parent interface,
\f[CI]BaseAudioContext\f[I].\f[R]
.TP
\f[B]OfflineAudioContext.suspend()\f[R]
Schedules a suspension of the time progression in the audio context at
the specified time and returns a promise.
.TP
\f[B]OfflineAudioContext.startRendering()\f[R]
Starts rendering the audio, taking into account the current connections
and the current scheduled changes.
This page covers both the event\-based version and the promise\-based
version.
.SS Deprecated methods
.TP
\f[B]OfflineAudioContext.resume()\f[R]
Resumes the progression of time in an audio context that has previously
been suspended.
.RS
.PP
\f[B]Note:\f[R] The \f[CR]resume()\f[R] method is still available \[em]
it is now defined on the \f[CR]BaseAudioContext\f[R] interface (see
\f[CR]AudioContext.resume\f[R]) and thus can be accessed by both the
\f[CR]AudioContext\f[R] and \f[CR]OfflineAudioContext\f[R] interfaces.
.RE
.SH EVENTS
Listen to these events using \f[CR]addEventListener()\f[R] or by
assigning an event listener to the \f[CR]oneventname\f[R] property of
this interface:
.TP
\f[B]complete\f[R]
Fired when the rendering of an offline audio context is complete.
.SH EXAMPLES
.SS Playing audio with an offline audio context
In this example, we declare both an \f[CR]AudioContext\f[R] and an
\f[CR]OfflineAudioContext\f[R] object.
We use the \f[CR]AudioContext\f[R] to load an audio track
\f[CR]fetch()\f[R], then the \f[CR]OfflineAudioContext\f[R] to render
the audio into an \f[CR]AudioBufferSourceNode\f[R] and play the track
through.
After the offline audio graph is set up, we render it to an
\f[CR]AudioBuffer\f[R] using
\f[CR]OfflineAudioContext.startRendering()\f[R].
.PP
When the \f[CR]startRendering()\f[R] promise resolves, rendering has
completed and the output \f[CR]AudioBuffer\f[R] is returned out of the
promise.
.PP
At this point we create another audio context, create an
\f[CR]AudioBufferSourceNode\f[R] inside it, and set its buffer to be
equal to the promise \f[CR]AudioBuffer\f[R].
This is then played as part of a simple standard audio graph.
.RS
.PP
\f[B]Note:\f[R] You can \c
.UR https://mdn.github.io/webaudio-examples/offline-audio-context-promise/
run the full example live
.UE \c
, or \c
.UR https://github.com/mdn/webaudio-examples/blob/main/offline-audio-context-promise/
view the source
.UE \c
\&.
.RE
.IP
.EX
\f[I]// Define both online and offline audio contexts\f[R]
\f[B]let\f[R] audioCtx; \f[I]// Must be initialized after a user interaction\f[R]
\f[B]const\f[R] offlineCtx = \f[B]new\f[R] OfflineAudioContext(2, 44100 * 40, 44100);

\f[I]// Define constants for dom nodes\f[R]
\f[B]const\f[R] play = document.querySelector(\[dq]#play\[dq]);

\f[B]function\f[R] getData() {
  \f[I]// Fetch an audio track, decode it and stick it in a buffer.\f[R]
  \f[I]// Then we put the buffer into the source and can play it.\f[R]
  fetch(\[dq]viper.ogg\[dq])
    .then((response) \f[B]=>\f[R] response.arrayBuffer())
    .then((downloadedBuffer) \f[B]=>\f[R] audioCtx.decodeAudioData(downloadedBuffer))
    .then((decodedBuffer) \f[B]=>\f[R] {
      console.log(\[dq]File downloaded successfully.\[dq]);
      \f[B]const\f[R] source = \f[B]new\f[R] AudioBufferSourceNode(offlineCtx, {
        buffer: decodedBuffer,
      });
      source.connect(offlineCtx.destination);
      \f[B]return\f[R] source.start();
    })
    .then(() \f[B]=>\f[R] offlineCtx.startRendering())
    .then((renderedBuffer) \f[B]=>\f[R] {
      console.log(\[dq]Rendering completed successfully.\[dq]);
      play.disabled = \f[B]false\f[R];
      \f[B]const\f[R] song = \f[B]new\f[R] AudioBufferSourceNode(audioCtx, {
        buffer: renderedBuffer,
      });
      song.connect(audioCtx.destination);

      \f[I]// Start the song\f[R]
      song.start();
    })
    .catch((err) \f[B]=>\f[R] {
      console.error(\[ga]Error encountered: ${err}\[ga]);
    });
}

\f[I]// Activate the play button\f[R]
play.onclick = () \f[B]=>\f[R] {
  play.disabled = \f[B]true\f[R];
  \f[I]// We can initialize the context as the user clicked.\f[R]
  audioCtx = \f[B]new\f[R] AudioContext();

  \f[I]// Fetch the data and start the song\f[R]
  getData();
};
.EE
.SH SEE ALSO
.IP \[bu] 2
Using the Web Audio API
