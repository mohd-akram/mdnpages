.\" Automatically generated by Pandoc 3.2.1
.\"
.TH "AudioBuffer.getChannelData" "JS" "July 21, 2024" "JavaScript" "JavaScript Reference Manual"
.SH NAME
AudioBuffer.getChannelData \- AudioBuffer: getChannelData() method
.SH SYNOPSIS
The \f[B]\f[CB]getChannelData()\f[B]\f[R] method of the
\f[CR]AudioBuffer\f[R] Interface returns a \f[CR]Float32Array\f[R]
containing the PCM data associated with the channel, defined by the
channel parameter (with 0 representing the first channel).
.SH SYNTAX
.IP
.EX
getChannelData(channel)
.EE
.SS Parameters
.TP
\f[B]channel\f[R]
The channel property is an index representing the particular channel to
get data for.
An index value of 0 represents the first channel.
If the \f[CR]channel\f[R] index value is greater than of equal to
\f[CR]AudioBuffer.numberOfChannels\f[R], an \f[CR]INDEX_SIZE_ERR\f[R]
exception will be thrown.
.SS Return value
A \f[CR]Float32Array\f[R].
.SH EXAMPLES
In the following example we create a two second buffer, fill it with
white noise, and then play it via an \f[CR]AudioBufferSourceNode\f[R].
The comments should clearly explain what is going on.
You can also \c
.UR https://mdn.github.io/webaudio-examples/audio-buffer/
run the code live
.UE \c
, or \c
.UR https://github.com/mdn/webaudio-examples
view the source
.UE \c
\&.
.IP
.EX
\f[B]const\f[R] audioCtx = \f[B]new\f[R] AudioContext();
\f[B]const\f[R] button = document.querySelector(\[dq]button\[dq]);
\f[B]const\f[R] pre = document.querySelector(\[dq]pre\[dq]);
\f[B]const\f[R] myScript = document.querySelector(\[dq]script\[dq]);

pre.textContent = myScript.textContent;

\f[I]// Stereo\f[R]
\f[B]const\f[R] channels = 2;
\f[I]// Create an empty two second stereo buffer at the\f[R]
\f[I]// sample rate of the AudioContext\f[R]
\f[B]const\f[R] frameCount = audioCtx.sampleRate * 2.0;

\f[B]const\f[R] myArrayBuffer = audioCtx.createBuffer(2, frameCount, audioCtx.sampleRate);

button.onclick = () \f[B]=>\f[R] {
  \f[I]// Fill the buffer with white noise;\f[R]
  \f[I]//just random values between \-1.0 and 1.0\f[R]
  \f[B]for\f[R] (\f[B]let\f[R] channel = 0; channel < channels; channel++) {
    \f[I]// This gives us the actual ArrayBuffer that contains the data\f[R]
    \f[B]const\f[R] nowBuffering = myArrayBuffer.getChannelData(channel);
    \f[B]for\f[R] (\f[B]let\f[R] i = 0; i < frameCount; i++) {
      \f[I]// Math.random() is in [0; 1.0]\f[R]
      \f[I]// audio needs to be in [\-1.0; 1.0]\f[R]
      nowBuffering[i] = Math.random() * 2 \- 1;
    }
  }

  \f[I]// Get an AudioBufferSourceNode.\f[R]
  \f[I]// This is the AudioNode to use when we want to play an AudioBuffer\f[R]
  \f[B]const\f[R] source = audioCtx.createBufferSource();
  \f[I]// set the buffer in the AudioBufferSourceNode\f[R]
  source.buffer = myArrayBuffer;
  \f[I]// connect the AudioBufferSourceNode to the\f[R]
  \f[I]// destination so we can hear the sound\f[R]
  source.connect(audioCtx.destination);
  \f[I]// start the source playing\f[R]
  source.start();
};
.EE
.SH SEE ALSO
.IP \[bu] 2
Using the Web Audio API
