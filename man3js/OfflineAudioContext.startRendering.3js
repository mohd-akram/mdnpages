.\" Automatically generated by Pandoc 3.1.12.3
.\"
.TH "OfflineAudioContext.startRendering" "JS" "November 21, 2023" "JavaScript" "JavaScript Reference Manual"
.SH NAME
OfflineAudioContext.startRendering \- OfflineAudioContext:
startRendering() method
.SH SYNOPSIS
The \f[CR]startRendering()\f[R] method of the
\f[CR]OfflineAudioContext\f[R] Interface starts rendering the audio
graph, taking into account the current connections and the current
scheduled changes.
.PP
The \f[CR]complete\f[R] event (of type
\f[CR]OfflineAudioCompletionEvent\f[R]) is raised when the rendering is
finished, containing the resulting \f[CR]AudioBuffer\f[R] in its
\f[CR]renderedBuffer\f[R] property.
.PP
Browsers currently support two versions of the
\f[CR]startRendering()\f[R] method \[em] an older event\-based version
and a newer promise\-based version.
The former will eventually be removed, but currently both mechanisms are
provided for legacy reasons.
.SH SYNTAX
.IP
.EX
startRendering()
.EE
.SS Parameters
None.
.SS Return value
A \f[CR]Promise\f[R] that fulfills with an \f[CR]AudioBuffer\f[R].
.SH EXAMPLES
.SS Playing audio with an offline audio context
In this example, we declare both an \f[CR]AudioContext\f[R] and an
\f[CR]OfflineAudioContext\f[R] object.
We use the \f[CR]AudioContext\f[R] to load an audio track
\f[CR]fetch()\f[R], then the \f[CR]OfflineAudioContext\f[R] to render
the audio into an \f[CR]AudioBufferSourceNode\f[R] and play the track
through.
After the offline audio graph is set up, we render it to an
\f[CR]AudioBuffer\f[R] using
\f[CR]OfflineAudioContext.startRendering()\f[R].
.PP
When the \f[CR]startRendering()\f[R] promise resolves, rendering has
completed and the output \f[CR]AudioBuffer\f[R] is returned out of the
promise.
.PP
At this point we create another audio context, create an
\f[CR]AudioBufferSourceNode\f[R] inside it, and set its buffer to be
equal to the promise \f[CR]AudioBuffer\f[R].
This is then played as part of a simple standard audio graph.
.RS
.PP
\f[B]Note:\f[R] You can \c
.UR https://mdn.github.io/webaudio-examples/offline-audio-context-promise/
run the full example live
.UE \c
, or \c
.UR https://github.com/mdn/webaudio-examples/blob/main/offline-audio-context-promise/
view the source
.UE \c
\&.
.RE
.IP
.EX
\f[I]// Define both online and offline audio contexts\f[R]
\f[B]let\f[R] audioCtx; \f[I]// Must be initialized after a user interaction\f[R]
\f[B]const\f[R] offlineCtx = \f[B]new\f[R] OfflineAudioContext(2, 44100 * 40, 44100);

\f[I]// Define constants for dom nodes\f[R]
\f[B]const\f[R] play = document.querySelector(\[dq]#play\[dq]);

\f[B]function\f[R] getData() {
  \f[I]// Fetch an audio track, decode it and stick it in a buffer.\f[R]
  \f[I]// Then we put the buffer into the source and can play it.\f[R]
  fetch(\[dq]viper.ogg\[dq])
    .then((response) \f[B]=>\f[R] response.arrayBuffer())
    .then((downloadedBuffer) \f[B]=>\f[R] audioCtx.decodeAudioData(downloadedBuffer))
    .then((decodedBuffer) \f[B]=>\f[R] {
      console.log(\[dq]File downloaded successfully.\[dq]);
      \f[B]const\f[R] source = \f[B]new\f[R] AudioBufferSourceNode(offlineCtx, {
        buffer: decodedBuffer,
      });
      source.connect(offlineCtx.destination);
      \f[B]return\f[R] source.start();
    })
    .then(() \f[B]=>\f[R] offlineCtx.startRendering())
    .then((renderedBuffer) \f[B]=>\f[R] {
      console.log(\[dq]Rendering completed successfully.\[dq]);
      play.disabled = \f[B]false\f[R];
      \f[B]const\f[R] song = \f[B]new\f[R] AudioBufferSourceNode(audioCtx, {
        buffer: renderedBuffer,
      });
      song.connect(audioCtx.destination);

      \f[I]// Start the song\f[R]
      song.start();
    })
    .catch((err) \f[B]=>\f[R] {
      console.error(\[ga]Error encountered: ${err}\[ga]);
    });
}

\f[I]// Activate the play button\f[R]
play.onclick = () \f[B]=>\f[R] {
  play.disabled = \f[B]true\f[R];
  \f[I]// We can initialize the context as the user clicked.\f[R]
  audioCtx = \f[B]new\f[R] AudioContext();

  \f[I]// Fetch the data and start the song\f[R]
  getData();
};
.EE
.SH SEE ALSO
.IP \[bu] 2
Using the Web Audio API
