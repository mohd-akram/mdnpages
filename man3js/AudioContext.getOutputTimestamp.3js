.\" Automatically generated by Pandoc 3.6.2
.\"
.TH "AudioContext.getOutputTimestamp" "JS" "November 28, 2023" "JavaScript" "JavaScript Reference Manual"
.SH NAME
AudioContext.getOutputTimestamp \- AudioContext: getOutputTimestamp()
method
.SH SYNOPSIS
The \f[B]\f[CB]getOutputTimestamp()\f[B]\f[R] method of the
\f[CR]AudioContext\f[R] interface returns a new
\f[CR]AudioTimestamp\f[R] object containing two audio timestamp values
relating to the current audio context.
.PP
The two values are as follows:
.IP \[bu] 2
\f[CR]AudioTimestamp.contextTime\f[R]: The time of the sample frame
currently being rendered by the audio output device (i.e., output audio
stream position), in the same units and origin as the context\[cq]s
\f[CR]AudioContext.currentTime\f[R].
Basically, this is the time after the audio context was first created.
.IP \[bu] 2
\f[CR]AudioTimestamp.performanceTime\f[R]: An estimation of the moment
when the sample frame corresponding to the stored \f[CR]contextTime\f[R]
value was rendered by the audio output device, in the same units and
origin as \f[CR]performance.now()\f[R].
This is the time after the document containing the audio context was
first rendered.
.SH SYNTAX
.IP
.EX
getOutputTimestamp()
.EE
.SS Parameters
None.
.SS Return value
An \f[CR]AudioTimestamp\f[R] object, which has the following properties.
.IP \[bu] 2
\f[CR]contextTime\f[R]: A point in the time coordinate system of the
\f[CR]currentTime\f[R] for the \f[CR]BaseAudioContext\f[R]; the time
after the audio context was first created.
.IP \[bu] 2
\f[CR]performanceTime\f[R]: A point in the time coordinate system of a
\f[CR]Performance\f[R] interface; the time after the document containing
the audio context was first rendered
.SH EXAMPLES
In the following code we start to play an audio file after a play button
is clicked, and start off a \f[CR]requestAnimationFrame\f[R] loop
running, which constantly outputs the \f[CR]contextTime\f[R] and
\f[CR]performanceTime\f[R].
.PP
You can see full code of this \c
.UR https://github.com/mdn/webaudio-examples/blob/main/output-timestamp/index.html
example at output\-timestamp
.UE \c
\ (\c
.UR https://mdn.github.io/webaudio-examples/output-timestamp/
see it live also
.UE \c
).
.IP
.EX
\f[I]// Press the play button\f[R]
playBtn.addEventListener(\[dq]click\[dq], () \f[B]=>\f[R] {
  \f[I]// We can create the audioCtx as there has been some user action\f[R]
  \f[B]if\f[R] (!audioCtx) {
    audioCtx = \f[B]new\f[R] AudioContext();
  }
  source = \f[B]new\f[R] AudioBufferSourceNode(audioCtx);
  getData();
  source.start(0);
  playBtn.disabled = \f[B]true\f[R];
  stopBtn.disabled = \f[B]false\f[R];
  rAF = requestAnimationFrame(outputTimestamps);
});

\f[I]// Press the stop button\f[R]
stopBtn.addEventListener(\[dq]click\[dq], () \f[B]=>\f[R] {
  source.stop(0);
  playBtn.disabled = \f[B]false\f[R];
  stopBtn.disabled = \f[B]true\f[R];
  cancelAnimationFrame(rAF);
});

\f[I]// Helper function to output timestamps\f[R]
\f[B]function\f[R] outputTimestamps() {
  \f[B]const\f[R] ts = audioCtx.getOutputTimestamp();
  output.textContent = \[ga]Context time: ${ts.contextTime} | Performance time: ${ts.performanceTime}\[ga];
  rAF = requestAnimationFrame(outputTimestamps); \f[I]// Reregister itself\f[R]
}
.EE
