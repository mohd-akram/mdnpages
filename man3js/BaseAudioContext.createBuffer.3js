.\" Automatically generated by Pandoc 3.2.1
.\"
.TH "BaseAudioContext.createBuffer" "JS" "August 3, 2024" "JavaScript" "JavaScript Reference Manual"
.SH NAME
BaseAudioContext.createBuffer \- BaseAudioContext: createBuffer() method
.SH SYNOPSIS
The \f[CR]createBuffer()\f[R] method of the \f[CR]BaseAudioContext\f[R]
Interface is used to create a new, empty \f[CR]AudioBuffer\f[R] object,
which can then be populated by data, and played via an
\f[CR]AudioBufferSourceNode\f[R].
.PP
For more details about audio buffers, check out the
\f[CR]AudioBuffer\f[R] reference page.
.RS
.PP
\f[B]Note:\f[R] \f[CR]createBuffer()\f[R] used to be able to take
compressed data and give back decoded samples, but this ability was
removed from the specification, because all the decoding was done on the
main thread, so \f[CR]createBuffer()\f[R] was blocking other code
execution.
The asynchronous method \f[CR]decodeAudioData()\f[R] does the same thing
\[em] takes compressed audio, such as an MP3 file, and directly gives
you back an \f[CR]AudioBuffer\f[R] that you can then play via an
\f[CR]AudioBufferSourceNode\f[R].
For simple use cases like playing an MP3, \f[CR]decodeAudioData()\f[R]
is what you should be using.
.RE
.SH SYNTAX
.IP
.EX
createBuffer(numOfChannels, length, sampleRate)
.EE
.SS Parameters
.RS
.PP
\f[B]Note:\f[R] For an in\-depth explanation of how audio buffers work,
and what these parameters mean, read Audio buffers: frames, samples and
channels from our Basic concepts guide.
.RE
.TP
\f[B]numOfChannels\f[R]
An integer representing the number of channels this buffer should have.
The default value is 1, and all user agents must support at least 32
channels.
.TP
\f[B]length\f[R]
An integer representing the size of the buffer in sample\-frames (where
each sample\-frame is the size of a sample in bytes multiplied by
\f[CR]numOfChannels\f[R]).
To determine the \f[CR]length\f[R] to use for a specific number of
seconds of audio, use \f[CR]numSeconds * sampleRate\f[R].
.TP
\f[B]sampleRate\f[R]
The sample rate of the linear audio data in sample\-frames per second.
All browsers must support sample rates in at least the range 8,000 Hz to
96,000 Hz.
.SS Return value
An \f[CR]AudioBuffer\f[R] configured based on the specified options.
.SS Exceptions
.TP
\f[B]NotSupportedError\f[R] \f[B]DOMException\f[R]
Thrown if one or more of the options are negative or otherwise has an
invalid value (such as \f[CR]numberOfChannels\f[R] being higher than
supported, or a \f[CR]sampleRate\f[R] outside the nominal range).
.TP
\f[B]RangeError\f[R]
Thrown if there isn\[cq]t enough memory available to allocate the
buffer.
.SH EXAMPLES
First, a couple of simple trivial examples, to help explain how the
parameters are used:
.IP
.EX
\f[B]const\f[R] audioCtx = \f[B]new\f[R] AudioContext();
\f[B]const\f[R] buffer = audioCtx.createBuffer(2, 22050, 44100);
.EE
.PP
If you use this call, you will get a stereo buffer (two channels), that,
when played back on an AudioContext running at 44100Hz (very common,
most normal sound cards run at this rate), will last for 0.5 seconds:
22050 frames / 44100Hz = 0.5 seconds.
.IP
.EX
\f[B]const\f[R] audioCtx = \f[B]new\f[R] AudioContext();
\f[B]const\f[R] buffer = audioCtx.createBuffer(1, 22050, 22050);
.EE
.PP
If you use this call, you will get a mono buffer (one channel), that,
when played back on an \f[CR]AudioContext\f[R] running at 44100Hz, will
be automatically \f[I]resampled\f[R] to 44100Hz (and therefore yield
44100 frames), and last for 1.0 second: 44100 frames / 44100Hz = 1
second.
.RS
.PP
\f[B]Note:\f[R] Audio resampling is very similar to image resizing: say
you\[cq]ve got a 16 x 16 image, but you want it to fill a 32x32 area:
you resize (resample) it.
the result has less quality (it can be blurry or edgy, depending on the
resizing algorithm), but it works, and the resized image takes up less
space.
Resampled audio is exactly the same \[em] you save space, but in
practice you will be unable to properly reproduce high frequency content
(treble sound).
.RE
.PP
Now let\[cq]s look at a more complex \f[CR]createBuffer()\f[R] example,
in which we create a three\-second buffer, fill it with white noise, and
then play it via an \f[CR]AudioBufferSourceNode\f[R].
The comment should clearly explain what is going on.
You can also \c
.UR https://mdn.github.io/webaudio-examples/audio-buffer/
run the code live
.UE \c
, or \c
.UR https://github.com/mdn/webaudio-examples/blob/main/audio-buffer/index.html
view the source
.UE \c
\&.
.IP
.EX
\f[B]const\f[R] audioCtx = \f[B]new\f[R] AudioContext();

\f[I]// Create an empty three\-second stereo buffer at the sample rate of the AudioContext\f[R]
\f[B]const\f[R] myArrayBuffer = audioCtx.createBuffer(
  2,
  audioCtx.sampleRate * 3,
  audioCtx.sampleRate,
);

\f[I]// Fill the buffer with white noise;\f[R]
\f[I]// just random values between \-1.0 and 1.0\f[R]
\f[B]for\f[R] (\f[B]let\f[R] channel = 0; channel < myArrayBuffer.numberOfChannels; channel++) {
  \f[I]// This gives us the actual ArrayBuffer that contains the data\f[R]
  \f[B]const\f[R] nowBuffering = myArrayBuffer.getChannelData(channel);
  \f[B]for\f[R] (\f[B]let\f[R] i = 0; i < myArrayBuffer.length; i++) {
    \f[I]// Math.random() is in [0; 1.0]\f[R]
    \f[I]// audio needs to be in [\-1.0; 1.0]\f[R]
    nowBuffering[i] = Math.random() * 2 \- 1;
  }
}

\f[I]// Get an AudioBufferSourceNode.\f[R]
\f[I]// This is the AudioNode to use when we want to play an AudioBuffer\f[R]
\f[B]const\f[R] source = audioCtx.createBufferSource();
\f[I]// set the buffer in the AudioBufferSourceNode\f[R]
source.buffer = myArrayBuffer;
\f[I]// connect the AudioBufferSourceNode to the\f[R]
\f[I]// destination so we can hear the sound\f[R]
source.connect(audioCtx.destination);
\f[I]// start the source playing\f[R]
source.start();
.EE
.SH SEE ALSO
.IP \[bu] 2
Using the Web Audio API
